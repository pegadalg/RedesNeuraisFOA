{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Para demonstrar a diferença de capacidade entre uma rede com um único neurônio e uma rede com dois neurônios em duas camadas, precisamos de um dataset que não seja linearmente separável. Um exemplo clássico é o problema do XOR, que já usamos anteriormente, mas para deixar mais claro, vou criar um dataset um pouco mais complexo: um círculo dentro de outro círculo (um problema de classificação circular).\n",
        "\n",
        "Dataset: Círculos Concêntricos\n",
        "Vamos gerar um dataset onde os pontos de uma classe estão dentro de um círculo e os pontos da outra classe estão em um anel ao redor desse círculo. Esse tipo de problema não pode ser resolvido por um único neurônio (que só pode aprender fronteiras lineares), mas pode ser resolvido por uma rede com dois neurônios em duas camadas (que pode aprender fronteiras não lineares).\n",
        "\n",
        "Abaixo está o código que gera o dataset, treina as duas redes (uma com um neurônio e outra com dois neurônios em duas camadas) e mostra que a primeira rede não consegue aprender o problema, enquanto a segunda consegue:"
      ],
      "metadata": {
        "id": "1tCVxvxRVRDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "\n",
        "# Funções de ativação\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Gerar dataset de círculos concêntricos\n",
        "X, y = make_circles(n_samples=100, noise=0.1, factor=0.5, random_state=42)\n",
        "y = y.reshape(-1, 1)  # Transformar y em formato coluna\n",
        "\n",
        "# Plot do dataset\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y.flatten(), cmap=plt.cm.Spectral)\n",
        "plt.title(\"Dataset: Círculos Concêntricos\")\n",
        "plt.show()\n",
        "\n",
        "# Hiperparâmetros\n",
        "learning_rate = 0.1\n",
        "epochs = 80000\n",
        "\n",
        "# ==============================================\n",
        "# Rede com um único neurônio\n",
        "# ==============================================\n",
        "np.random.seed(42)\n",
        "weights_1 = np.random.randn(2, 1)\n",
        "bias_1 = np.random.randn()\n",
        "\n",
        "errors_1 = []\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    weighted_sum = np.dot(X, weights_1) + bias_1\n",
        "    output = sigmoid(weighted_sum)\n",
        "\n",
        "    # Cálculo do erro\n",
        "    error = y - output\n",
        "    errors_1.append(np.mean(np.square(error)))\n",
        "\n",
        "    # Backpropagation\n",
        "    d_error = -2 * error / len(X)\n",
        "    d_output = sigmoid_derivative(weighted_sum)\n",
        "    gradient = d_error * d_output\n",
        "\n",
        "    weights_1 -= learning_rate * np.dot(X.T, gradient)\n",
        "    bias_1 -= learning_rate * np.sum(gradient)\n",
        "\n",
        "# Plot do erro da rede com um neurônio\n",
        "plt.plot(range(epochs), errors_1, label=\"Rede com 1 neurônio\")\n",
        "plt.title(\"Erro ao longo do treinamento\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Erro\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Teste da rede com um neurônio\n",
        "print(\"\\nTeste da rede com um neurônio:\")\n",
        "predictions_1 = sigmoid(np.dot(X, weights_1) + bias_1)\n",
        "predictions_1 = (predictions_1 > 0.5).astype(int)\n",
        "print(f\"Acurácia: {np.mean(predictions_1 == y) * 100:.2f}%\")\n",
        "\n",
        "# ==============================================\n",
        "# Rede com dois neurônios em duas camadas\n",
        "# ==============================================\n",
        "np.random.seed(42)\n",
        "# Camada 1: 2 entradas -> 2 neurônios\n",
        "weights_1_layer1 = np.random.randn(2, 2)\n",
        "bias_1_layer1 = np.random.randn(1, 2)\n",
        "\n",
        "# Camada 2: 2 entradas (saída da camada 1) -> 1 neurônio\n",
        "weights_1_layer2 = np.random.randn(2, 1)\n",
        "bias_1_layer2 = np.random.randn()\n",
        "\n",
        "errors_2 = []\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    # Camada 1\n",
        "    weighted_sum_1 = np.dot(X, weights_1_layer1) + bias_1_layer1\n",
        "    output_1 = sigmoid(weighted_sum_1)\n",
        "\n",
        "    # Camada 2\n",
        "    weighted_sum_2 = np.dot(output_1, weights_1_layer2) + bias_1_layer2\n",
        "    output_2 = sigmoid(weighted_sum_2)\n",
        "\n",
        "    # Cálculo do erro\n",
        "    error = y - output_2\n",
        "    errors_2.append(np.mean(np.square(error)))\n",
        "\n",
        "    # Backpropagation\n",
        "    # Gradiente da camada 2\n",
        "    d_error = -2 * error / len(X)\n",
        "    d_output_2 = sigmoid_derivative(weighted_sum_2)\n",
        "    gradient_2 = d_error * d_output_2\n",
        "\n",
        "    # Gradiente da camada 1\n",
        "    d_output_1 = sigmoid_derivative(weighted_sum_1)\n",
        "    gradient_1 = np.dot(gradient_2, weights_1_layer2.T) * d_output_1\n",
        "\n",
        "    # Atualização dos pesos e viés\n",
        "    weights_1_layer2 -= learning_rate * np.dot(output_1.T, gradient_2)\n",
        "    bias_1_layer2 -= learning_rate * np.sum(gradient_2, axis=0)\n",
        "\n",
        "    weights_1_layer1 -= learning_rate * np.dot(X.T, gradient_1)\n",
        "    bias_1_layer1 -= learning_rate * np.sum(gradient_1, axis=0)\n",
        "\n",
        "# Plot do erro da rede com dois neurônios\n",
        "plt.plot(range(epochs), errors_2, label=\"Rede com 2 neurônios\", color=\"orange\")\n",
        "plt.title(\"Erro ao longo do treinamento\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Erro\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Teste da rede com dois neurônios\n",
        "print(\"\\nTeste da rede com dois neurônios:\")\n",
        "output_1 = sigmoid(np.dot(X, weights_1_layer1) + bias_1_layer1)\n",
        "output_2 = sigmoid(np.dot(output_1, weights_1_layer2) + bias_1_layer2)\n",
        "predictions_2 = (output_2 > 0.5).astype(int)\n",
        "print(f\"Acurácia: {np.mean(predictions_2 == y) * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "Su55aUAsVd6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusão\n",
        "Esse exemplo mostra claramente a importância de adicionar mais neurônios e camadas para resolver problemas não linearmente separáveis. A rede com um único neurônio não tem capacidade suficiente para aprender o dataset, enquanto a rede com dois neurônios em duas camadas consegue."
      ],
      "metadata": {
        "id": "hHw3onVXVxnI"
      }
    }
  ]
}