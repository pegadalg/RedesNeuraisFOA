{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Código equivalente ao código anterior, mas agora com uma rede neural de duas camadas, onde cada camada tem um neurônio. A primeira camada recebe as entradas, e a segunda camada recebe a saída da primeira camada como entrada. Vamos usar o mesmo dataset fictício (problema XOR) e permitir a escolha de funções de ativação."
      ],
      "metadata": {
        "id": "ytVHkVA7UMoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Funções de ativação\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "# Derivadas das funções de ativação (para o gradiente)\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - np.tanh(x)**2\n",
        "\n",
        "# Dataset fictício (problema de classificação binária)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Entradas\n",
        "y = np.array([[0], [1], [1], [0]])              # Saídas esperadas (XOR)\n",
        "\n",
        "# Hiperparâmetros\n",
        "learning_rate = 0.1\n",
        "epochs = 10000\n",
        "\n",
        "# Inicialização dos pesos e viés\n",
        "np.random.seed(42)\n",
        "# Camada 1: 2 entradas -> 1 neurônio\n",
        "weights_1 = np.random.randn(2, 1)\n",
        "bias_1 = np.random.randn()\n",
        "\n",
        "# Camada 2: 1 entrada (saída da camada 1) -> 1 neurônio\n",
        "weights_2 = np.random.randn(1, 1)\n",
        "bias_2 = np.random.randn()\n",
        "\n",
        "# Escolha da função de ativação\n",
        "activation_function = sigmoid\n",
        "activation_derivative = sigmoid_derivative\n",
        "\n",
        "# Treinamento da rede\n",
        "errors = []\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    # Camada 1\n",
        "    weighted_sum_1 = np.dot(X, weights_1) + bias_1\n",
        "    output_1 = activation_function(weighted_sum_1)\n",
        "\n",
        "    # Camada 2\n",
        "    weighted_sum_2 = np.dot(output_1, weights_2) + bias_2\n",
        "    output_2 = activation_function(weighted_sum_2)\n",
        "\n",
        "    # Cálculo do erro (erro quadrático médio)\n",
        "    error = y - output_2\n",
        "    errors.append(np.mean(np.square(error)))\n",
        "\n",
        "    # Backpropagation\n",
        "    # Gradiente da camada 2\n",
        "    d_error = -2 * error / len(X)\n",
        "    d_output_2 = activation_derivative(weighted_sum_2)\n",
        "    gradient_2 = d_error * d_output_2\n",
        "\n",
        "    # Gradiente da camada 1\n",
        "    d_output_1 = activation_derivative(weighted_sum_1)\n",
        "    gradient_1 = np.dot(gradient_2, weights_2.T) * d_output_1\n",
        "\n",
        "    # Atualização dos pesos e viés\n",
        "    weights_2 -= learning_rate * np.dot(output_1.T, gradient_2)\n",
        "    bias_2 -= learning_rate * np.sum(gradient_2)\n",
        "\n",
        "    weights_1 -= learning_rate * np.dot(X.T, gradient_1)\n",
        "    bias_1 -= learning_rate * np.sum(gradient_1)\n",
        "\n",
        "    # Exibir progresso a cada 1000 épocas\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Época {epoch}, Erro: {errors[-1]}\")\n"
      ],
      "metadata": {
        "id": "egTUSccKUTWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot do erro ao longo do treinamento\n",
        "plt.plot(range(epochs), errors)\n",
        "plt.title(\"Erro ao longo do treinamento\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Erro\")\n",
        "plt.show()\n",
        "\n",
        "# Teste da rede treinada\n",
        "print(\"\\nTeste da rede treinada:\")\n",
        "for i in range(len(X)):\n",
        "    # Forward pass\n",
        "    weighted_sum_1 = np.dot(X[i], weights_1) + bias_1\n",
        "    output_1 = activation_function(weighted_sum_1)\n",
        "\n",
        "    weighted_sum_2 = np.dot(output_1, weights_2) + bias_2\n",
        "    output_2 = activation_function(weighted_sum_2)\n",
        "\n",
        "    print(f\"Entrada: {X[i]}, Saída esperada: {y[i]}, Saída da rede: {output_2}\")"
      ],
      "metadata": {
        "id": "_ABiebIyUrdo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}